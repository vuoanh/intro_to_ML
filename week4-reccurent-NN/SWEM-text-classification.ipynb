{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a52c3b5-6a7e-4848-905a-7dc6d84862dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from nltk.corpus import stopwords\n",
    "#from collections import Counter\n",
    "#import string\n",
    "#import re\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, load_dataset, load_dataset_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c3bfde-63e6-4184-aeb3-036667294ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# set the GPU device for M4 macbook air\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f8a58c-4189-44fc-aafb-41745c8351ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': SplitInfo(name='train', num_bytes=29832303, num_examples=120000, shard_lengths=None, dataset_name='ag_news'), 'test': SplitInfo(name='test', num_bytes=1880424, num_examples=7600, shard_lengths=None, dataset_name='ag_news')}\n",
      "{'text': Value('string'), 'label': ClassLabel(names=['World', 'Sports', 'Business', 'Sci/Tech'])}\n"
     ]
    }
   ],
   "source": [
    "# inspect the dataset\n",
    "ag_news_builder = load_dataset_builder(\"wangrongsheng/ag_news\")\n",
    "print(ag_news_builder.info.splits)\n",
    "print(ag_news_builder.info.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "907a904f-c8c4-47c9-9748-ab4e09c34f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into GPU\n",
    "ag_news_train = load_dataset(\"wangrongsheng/ag_news\", split=\"train\").with_format('torch', device=mps_device)\n",
    "#ag_news_train = load_dataset(\"wangrongsheng/ag_news\", split=\"train\")\n",
    "ag_news_test = load_dataset(\"wangrongsheng/ag_news\", split=\"test\").with_format('torch', device=mps_device)\n",
    "#ag_news_test = load_dataset(\"wangrongsheng/ag_news\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2f63054-4bc2-4b4b-85ff-e906f0ebb9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\",\n",
       " 'label': tensor(2)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_news_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0e15778-5cfd-4a33-9fea-ed6f680c4517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "# Apply the tokenizer to the entire dataset\n",
    "tokenized_train = ag_news_train.map(preprocess_function)\n",
    "tokenized_test = ag_news_test.map(preprocess_function)\n",
    "\n",
    "# Example for PyTorch DataLoader (requires specifying the format first)\n",
    "tokenized_train.set_format(\"torch\", columns=['input_ids', 'label'])\n",
    "tokenized_test.set_format(\"torch\", columns=['input_ids', 'label'])\n",
    "\n",
    "# Initialize the data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "708863e5-6b19-40cd-a7c6-0d5dfc0e9848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'torch',\n",
       " 'format_kwargs': {},\n",
       " 'columns': ['input_ids', 'label'],\n",
       " 'output_all_columns': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3db640b6-7511-4769-9fe9-82ae02ff8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Load the datasets into dataloaders\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(tokenized_train,\n",
    "                          shuffle=True,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          collate_fn=data_collator # The key step for padding\n",
    "                         )\n",
    "\n",
    "test_loader = DataLoader(tokenized_test,\n",
    "                          shuffle=True,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          collate_fn=data_collator # The key step for padding\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cae582d2-03c8-453b-8db4-81077abb340b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 130])\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    print(i['input_ids'].size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ebd67ad-6cf9-4080-9efc-8debbc171b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size_simple(dataset, text_column=\"text\"):\n",
    "    \"\"\"Get vocabulary size using simple tokenization\"\"\"\n",
    "    vocab = set()\n",
    "    \n",
    "    for example in dataset:\n",
    "        words = example[text_column].lower().split()\n",
    "        vocab.update(words)\n",
    "    \n",
    "    return len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec491263-bb73-48d2-8c80-4548917f52db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158733"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vocab_size_simple(ag_news_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cca7c35f-e59f-44c0-aa78-1a22449a0c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# Load a pre-trained tokenizer (e.g., for 'bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04219bf4-759c-4b1c-b9dd-1f3eae968478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocab_size_custom_tokenizer(dataset, text_column=\"text\", target_vocab_size=10000):\n",
    "    \"\"\"Train custom tokenizer and return actual vocabulary size\"\"\"\n",
    "    from tokenizers import Tokenizer\n",
    "    from tokenizers.models import WordLevel\n",
    "    from tokenizers.trainers import WordLevelTrainer\n",
    "    from tokenizers.pre_tokenizers import Whitespace\n",
    "    \n",
    "    tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    \n",
    "    trainer = WordLevelTrainer(\n",
    "        vocab_size=target_vocab_size,\n",
    "        special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\"]\n",
    "    )\n",
    "    \n",
    "    def batch_iterator():\n",
    "        for example in dataset:\n",
    "            yield example[text_column]\n",
    "    \n",
    "    tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)\n",
    "    \n",
    "    return tokenizer.get_vocab_size()\n",
    "get_vocab_size_custom_tokenizer(ag_news_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2a5387f-d953-4b85-9bed-20a1d8cb54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "EMBED_DIM = 100\n",
    "HIDDEN_DIM = 64\n",
    "NUM_OUTPUTS = ag_news_train.features['label'].num_classes\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ccd37b-199b-4a57-b97e-e3bc60496603",
   "metadata": {},
   "source": [
    "### Simple Word Embedding Model\n",
    "First, let's try out the Simple Word Embedding Model (SWEM) that we built in Notebook 4A on the AG News dataset. Unlike before though, instead of loading pre-trained embeddings, let's learn the embeddings from scratch. Before we begin, it will be helpful to define a few more hyperparameters.\n",
    "\n",
    "Once again, we're going to organize our model as a `nn.Module`.\n",
    "Instead of assuming the input is already an embedding, we're going to make learning the embedding as part of our model.\n",
    "We do this by using `nn.Embedding` to perform an embedding look-up at the beginning of our forward pass.\n",
    "Once we've done the look up, we'll have a minibatch of embedded sequences of dimension $L \\times$ `BATCH_SIZE` $\\times$ `EMBED_DIM`.\n",
    "For SWEM, remember, we take the mean&ast; across the length dimension to get an average embedding for the sequence.\n",
    "\n",
    "<font size=\"1\"> \n",
    "&ast;Note: Technically we should only take the mean across the embeddings at the positions corresponding to \"real\" words in our input, and not for the zero paddings we artificially added.\n",
    "This can be done by generating a binary mask while doing the padding to track the \"real\" words in the input.\n",
    "Ultimately though, this refinement doesn't have much impact on the results for this particular task, so we omit it for simplicity.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ef4dc3b-3517-419a-89eb-6c6fc25e2287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SWEM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_dim, num_outputs):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        self.fc1 = nn.Linear(embedding_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        #print(f'embed: {embed.size()}')\n",
    "        embed_mean = torch.mean(embed, dim=1)\n",
    "        #print(f'embed_mean: {embed_mean.size()}')\n",
    "        h = self.fc1(embed_mean)\n",
    "        #print(f'fc1: {h.size()}')\n",
    "        h = F.relu(h)\n",
    "        h = self.fc2(h)\n",
    "        #print(f'fc2: {h.size()}')\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a491f8cd-a2af-4946-9a68-2a2f638c28be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Train Loss: 0.46851399540901184 \t Train Acc: 0.697516679763794\n",
      "Epoch: 2 \t Train Loss: 0.4480689764022827 \t Train Acc: 0.9099000096321106\n",
      "Epoch: 4 \t Train Loss: 0.1308743953704834 \t Train Acc: 0.9268666505813599\n",
      "Epoch: 6 \t Train Loss: 0.18553726375102997 \t Train Acc: 0.9365083575248718\n",
      "Epoch: 8 \t Train Loss: 0.11664336919784546 \t Train Acc: 0.9469833374023438\n",
      "Test accuracy: 0.9127631783485413\n"
     ]
    }
   ],
   "source": [
    "# instantiate, train, and evaluate\n",
    "## Training\n",
    "# Instantiate model\n",
    "model = SWEM(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_OUTPUTS)\n",
    "model.to(mps_device)\n",
    "\n",
    "# Binary cross-entropy (BCE) Loss and Adam Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Iterate through train set minibatchs \n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    correct = 0\n",
    "    num_examples = 0\n",
    "    for data in train_loader:\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        inputs = data['input_ids'].to(device='mps')\n",
    "        labels = data['labels'].to(device='mps')\n",
    "\n",
    "        y = model(inputs)\n",
    "        loss = criterion(y, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predictions = torch.argmax(y, dim=1)\n",
    "        correct += torch.sum((predictions == labels).float())\n",
    "        num_examples += len(inputs)\n",
    "    \n",
    "    # Print training progress\n",
    "    if epoch % 2 == 0:\n",
    "        acc = correct/num_examples\n",
    "        print(\"Epoch: {0} \\t Train Loss: {1} \\t Train Acc: {2}\".format(epoch, loss, acc))\n",
    "\n",
    "## Testing\n",
    "correct = 0\n",
    "num_test = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Iterate through test set minibatchs \n",
    "    for data in test_loader:\n",
    "        # Forward pass\n",
    "        inputs = data['input_ids'].to(device='mps')\n",
    "        labels = data['labels'].to(device='mps')\n",
    "        y = model(inputs)\n",
    "        predictions = torch.argmax(y, dim=1)\n",
    "        correct += torch.sum((predictions == labels).float())\n",
    "        num_test += len(inputs)\n",
    "    \n",
    "print('Test accuracy: {}'.format(correct/num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "636bd003-bedd-494c-a878-cace9717b559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Sci/Tech (class 3)\n",
      "\n",
      "Confidence scores:\n",
      "  World: 33.56%\n",
      "  Sports: 7.92%\n",
      "  Business: 16.52%\n",
      "  Sci/Tech: 42.00%\n"
     ]
    }
   ],
   "source": [
    "# Use the trained model to classify a text string\n",
    "string = \"The broadcaster's board agreed the decision at a meeting on Wednesday, hours before the deadline for countries to confirm whether they will join what's supposed to be a celebratory 70th anniversary edition of the song contest next May.\"\n",
    "def predict_text(text, model, tokenizer, class_names):\n",
    "      \"\"\"\n",
    "      Predict the class of a given text string\n",
    "      \n",
    "      Args:\n",
    "          text: Input string to classify\n",
    "          model: Trained SWEM model\n",
    "          tokenizer: The tokenizer used during training\n",
    "          class_names: List of class names (e.g., ['World', 'Sports', 'Business', 'Sci/Tech'])\n",
    "      \n",
    "      Returns:\n",
    "          predicted_class: Integer class index\n",
    "          predicted_label: String class name\n",
    "          probabilities: Probability distribution over classes\n",
    "      \"\"\"\n",
    "      # Tokenize\n",
    "      tokenized = tokenizer(text, truncation=True, return_tensors='pt')\n",
    "      input_ids = tokenized['input_ids']\n",
    "\n",
    "      # If using GPU, move to device\n",
    "      input_ids = input_ids.to(device='mps')\n",
    "\n",
    "      # Predict\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "          logits = model(input_ids)\n",
    "          predicted_class = torch.argmax(logits, dim=1).item()\n",
    "          probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "      predicted_label = class_names[predicted_class]\n",
    "\n",
    "      return predicted_class, predicted_label, probabilities[0]\n",
    "\n",
    "# Usage\n",
    "class_names = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "pred_class, pred_label, probs = predict_text(string, model, tokenizer,\n",
    "class_names)\n",
    "\n",
    "print(f\"Prediction: {pred_label} (class {pred_class})\")\n",
    "print(f\"\\nConfidence scores:\")\n",
    "for i, name in enumerate(class_names):\n",
    "  print(f\"  {name}: {probs[i].item():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca2787-1240-4591-b6fe-174dac4faad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
